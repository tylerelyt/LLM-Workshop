---
layout: default
title: Fine-tuning Data Construction
nav_order: 8
has_children: true
description: "Instruction fine-tuning and preference learning data pipelines"
---

# Chapter 7: Fine-tuning Data Construction
{: .no_toc }

From Few-shot format to Self-Instruct, from Alpaca to RLHF, master the complete fine-tuning data construction workflow.
{: .fs-6 .fw-300 }

## Table of Contents
{: .no_toc .text-delta }

1. TOC
{:toc}

---

## Chapter Overview

This chapter provides comprehensive data pipelines covering various data formats for instruction fine-tuning and preference learning.

### Core Modules

üìù **Few-shot Data**
{: .label .label-blue }
Construct instruction format from structured data
{: .fs-3 }

üîÑ **Self-Instruct**
{: .label .label-green }
Automated data generation and expansion
{: .fs-3 }

ü¶ô **Alpaca Format**
{: .label .label-purple }
Supervised fine-tuning data preparation
{: .fs-3 }

‚≠ê **RLHF Preference Data**
{: .label .label-yellow }
Preference pair construction for reward modeling
{: .fs-3 }

---

## Academic Papers

- [Self-Instruct: Aligning Language Models with Self Generated Instructions](https://arxiv.org/abs/2212.10560) - Wang et al., 2022
- [Stanford Alpaca: An Instruction-following LLaMA Model](https://github.com/tatsu-lab/stanford_alpaca)
- [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155) - Ouyang et al., 2022
- [Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/abs/2212.08073) - Bai et al., 2022

---

## Next Steps

After completing this chapter, we recommend proceeding to Chapter 8 to learn Fine-tuning Fundamentals.

